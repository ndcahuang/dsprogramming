fb_post <- fromJSON("https://graph.facebook.com/v2.12/136845026417486_1267145630054081/comments?pretty=0&limit=5000&after=MTUxNwZDZD&access_token=EAACEdEose0cBAGmzZCtfVL6EPbQjFbqdsrdda5E1zxeFk9dFNZAJk9tAU3vuGTDsE3dAjxgZApznms4bEFC7P7CsZBTcHwBgZBOZCtkYiHgMEze91w7ZCNU7U52N7EmAJQ1j2tgjxaxSZCHfcvJtaNCd3pLGZCXz7nA0AFZCegG3vFylArCs5sXGjoa8AM3C8w8iZA0gx35ggzCsgZDZD")
fb_comment <- as.character(fb_post$data$message)
comment_db <- Corpus(VectorSource(fb_comment), list(language = NA))
comment_db <- tm_map(comment_db, stripWhitespace)
comment_db <- tm_map(comment_db, removePunctuation)
comment_db <- tm_map(comment_db, removeNumbers)
comment_db <- tm_map(comment_db, function(word) {gsub("[A-Za-z0-9]", "", word)})
comment_db <- tm_map(comment_db, segmentCN, nature = TRUE)
comment_db <- tm_map(comment_db, function(sentence) {
noun <- lapply(sentence, function(w) {
w[names(w) == "n"]
})
unlist(noun)
})
comment_db <- Corpus(VectorSource(comment_db))
tdm <- TermDocumentMatrix(comment_db, control = list(wordLengths = c(2, Inf)))
m1 <- as.matrix(tdm)
v <- sort(rowSums(m1), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
par(family=("Heiti TC Light"))
wordcloud(d$word, d$freq, min.freq = 10, random.order = F,
ordered.colors = F,
colors = rainbow(length(row.names(m1))))
library(tmcn)
library(tm)
library(jsonlite)
library(wordcloud)
Sys.setlocale(category = "LC_ALL", locale = "cht")
fb_post <- fromJSON("https://graph.facebook.com/v2.12/136845026417486_1267145630054081/comments?pretty=0&limit=5000&after=MTUxNwZDZD&access_token=EAACEdEose0cBAGmzZCtfVL6EPbQjFbqdsrdda5E1zxeFk9dFNZAJk9tAU3vuGTDsE3dAjxgZApznms4bEFC7P7CsZBTcHwBgZBOZCtkYiHgMEze91w7ZCNU7U52N7EmAJQ1j2tgjxaxSZCHfcvJtaNCd3pLGZCXz7nA0AFZCegG3vFylArCs5sXGjoa8AM3C8w8iZA0gx35ggzCsgZDZD")
fb_comment <- as.character(fb_post$data$message)
comment_db <- Corpus(VectorSource(fb_comment), list(language = NA))
comment_db <- tm_map(comment_db, stripWhitespace)
comment_db <- tm_map(comment_db, removePunctuation)
comment_db <- tm_map(comment_db, removeNumbers)
comment_db <- tm_map(comment_db, function(word) {gsub("[A-Za-z0-9]", "", word)})
comment_db <- tm_map(comment_db, segmentCN, nature = TRUE)
comment_db <- tm_map(comment_db, function(sentence) {
noun <- lapply(sentence, function(w) {
w[names(w) == "n"]
})
unlist(noun)
})
comment_db <- Corpus(VectorSource(comment_db))
tdm <- TermDocumentMatrix(comment_db, control = list(wordLengths = c(2, Inf)))
m1 <- as.matrix(tdm)
v <- sort(rowSums(m1), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
par(family=("Heiti TC Light"))
wordcloud(d$word, d$freq, min.freq = 10, random.order = F,
ordered.colors = F,
colors = rainbow(length(row.names(m1))))
library(tmcn)
library(tm)
library(jsonlite)
library(wordcloud)
Sys.setlocale(category = "LC_ALL", locale = "cht")
fb_post <- fromJSON("https://graph.facebook.com/v2.12/136845026417486_1267145630054081/comments?pretty=0&limit=5000&after=MTUxNwZDZD&access_token=EAACEdEose0cBAGmzZCtfVL6EPbQjFbqdsrdda5E1zxeFk9dFNZAJk9tAU3vuGTDsE3dAjxgZApznms4bEFC7P7CsZBTcHwBgZBOZCtkYiHgMEze91w7ZCNU7U52N7EmAJQ1j2tgjxaxSZCHfcvJtaNCd3pLGZCXz7nA0AFZCegG3vFylArCs5sXGjoa8AM3C8w8iZA0gx35ggzCsgZDZD")
fb_comment <- as.character(fb_post$data$message)
comment_db <- Corpus(VectorSource(fb_comment), list(language = NA))
comment_db <- tm_map(comment_db, stripWhitespace)
comment_db <- tm_map(comment_db, removePunctuation)
comment_db <- tm_map(comment_db, removeNumbers)
comment_db <- tm_map(comment_db, function(word) {gsub("[A-Za-z0-9]", "", word)})
comment_db <- tm_map(comment_db, segmentCN, nature = TRUE)
comment_db <- tm_map(comment_db, function(sentence) {
noun <- lapply(sentence, function(w) {
w[names(w) == "n"]
})
unlist(noun)
})
comment_db <- Corpus(VectorSource(comment_db))
tdm <- TermDocumentMatrix(comment_db, control = list(wordLengths = c(2, Inf)))
m1 <- as.matrix(tdm)
v <- sort(rowSums(m1), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
par(family=("Heiti TC Light"))
wordcloud(d$word, d$freq, min.freq = 10, random.order = F,
ordered.colors = F,
colors = rainbow(length(row.names(m1))))
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
typeof(msg[2])
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
tdm <- TermDocumentMatrix(corpus, control = list(weighting = weightTfIdf, wordLengths = c(1, 5)))
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
View(tdm)
View(corpus)
Sys.setlocale(category = "LC_ALL", locale = "cht")
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
typeof(msg[2])
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(tdm)
print(tdm)
print(tdm[1])
View(tdm)
Encoding(msg) <- "UTF-8"
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
View(tdm)
View(corpus)
corpus[1]
inspect(corpus[1])
corpus <- Corpus(VectorSource(corpus))
View(tdm)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
View(tdm)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
View(corpus)
inspect(data.frame(tdm))
View(tdm)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source, encoding = "UTF-8")
msg <- as.character(raw$data$message)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
View(tdm)
Sys.setlocale(category='LC_ALL', locale='C')
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(data.frame(tdm))
Sys.setlocale(category='LC_ALL', locale='C')
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(tdm)
Sys.setlocale(category='LC_ALL', locale='ch')
Sys.setlocale(category='LC_ALL', locale='en')
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(tdm)
Sys.setlocale(category='LC_ALL', locale='cht')
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(tdm)
Encoding(corpus) <- "UTF-8"
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(tdm)
View(corpus)
corpus <- tm_map(corpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus, control =list(wordLengths = c(2, Inf)))
tdm <- TermDocumentMatrix(corpus, control =list(wordLengths = c(1, 5)))
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,1)))
View(tdm)
View(tdm)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
View(corpus)
inspect(tdm)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,1)))
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 5)))
inspect(tdm)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
inspect(tdm)
Sys.setlocale(category = "LC_CTYPE", locale = "en_US.UTF-8")
inspect(tdm)
corpus <- Corpus(VectorSource(msg_df), readerControl = list(language = "UTF-8"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
Sys.setlocale(locale="English")
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
df <-as.data.frame(inspect(tdm))
Sys.setlocale(locale="cht")
df
Sys.setlocale(locale="cht")
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
df <-as.data.frame(inspect(tdm))
Sys.setlocale(locale="cht")
corpus <- Corpus(VectorSource(msg_df), readerControl = list(language = "UTF-8"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
corpus <- tm_map(corpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
df <-as.data.frame(inspect(tdm))
str(tdm)
a <- "你好"
str(a)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg[1]
typeof(msg)
msg
temp <- Corpus(VectorSource(msg))
View(temp)
dtm <- DocumentTermMatrix(temp)
View(dtm)
Sys.setlocale(locale="English")
dtm <- DocumentTermMatrix(temp)
View(dtm)
Sys.setlocale(locale="cht")
dtm <- DocumentTermMatrix(temp)
library(tmcn)
library(tm)
library(jsonlite)
library(wordcloud)
fb_post <- fromJSON("https://graph.facebook.com/v3.0/136845026417486_1267145630054081/comments?pretty=0&limit=5000&after=MTUxNwZDZD&access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD")
fb_comment <- as.character(fb_post$data$message)
comment_db <- Corpus(VectorSource(fb_comment), list(language = NA))
comment_db <- tm_map(comment_db, stripWhitespace)
comment_db <- tm_map(comment_db, removePunctuation)
comment_db <- tm_map(comment_db, removeNumbers)
comment_db <- tm_map(comment_db, function(word) {gsub("[A-Za-z0-9]", "", word)})
comment_db <- tm_map(comment_db, segmentCN, nature = TRUE)
comment_db <- tm_map(comment_db, function(sentence) {
noun <- lapply(sentence, function(w) {
w[names(w) == "n"]
})
unlist(noun)
})
comment_db <- Corpus(VectorSource(comment_db))
tdm <- TermDocumentMatrix(comment_db, control = list(wordLengths = c(2, Inf)))
fb_post <- fromJSON("https://graph.facebook.com/v3.0/136845026417486_1267145630054081/comments?pretty=0&limit=5000&after=MTUxNwZDZD&access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD")
library(tmcn)
library(tm)
library(jsonlite)
library(wordcloud)
fb_post <- fromJSON("https://graph.facebook.com/v3.0/136845026417486_1267145630054081/comments?pretty=0&limit=5000&after=MTUxNwZDZD&access_token=EAACEdEose0cBAOyKb54nR25RK9jp9No1MaHgTzcsV0RiXaPW2Yg4YahZCXBqPtCqpLnSR1Q3xDa8gP6KUZAX0l3dpPlVhfQloK9TburKJJNJkt33aNUgCCIDsnQksokH6ZByUfaIVhg97ZCdMXTJp5FhKgheYO1aMvhsWfqm4jkRQhqTdatEzv81eKJZAom8ZD")
fb_comment <- as.character(fb_post$data$message)
comment_db <- Corpus(VectorSource(fb_comment), list(language = NA))
comment_db <- tm_map(comment_db, stripWhitespace)
comment_db <- tm_map(comment_db, removePunctuation)
comment_db <- tm_map(comment_db, removeNumbers)
comment_db <- tm_map(comment_db, function(word) {gsub("[A-Za-z0-9]", "", word)})
comment_db <- tm_map(comment_db, segmentCN, nature = TRUE)
comment_db <- tm_map(comment_db, function(sentence) {
noun <- lapply(sentence, function(w) {
w[names(w) == "n"]
})
unlist(noun)
})
comment_db <- Corpus(VectorSource(comment_db))
tdm <- TermDocumentMatrix(comment_db, control = list(wordLengths = c(2, Inf)))
View(tdm)
m1 <- as.matrix(tdm)
View(m1)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAEteZASDZAvpm0tH9G8MpzxqBmLjbT5cApJDDShyjErb8cixzkVxHRTZBt8fLKBZBDEYWFoA7xreOCVA30LdbhTo9Yi2kwQ4USL5lu1DK9GVAVJkpMAZBogD47zQePZAvoCIWySpJhfzwVKB3PVo1tL2YSeIVvlylkCRd7vihzlW3z3bkit4UZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df), readerControl = list(language = "UTF-8"))
orpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
corpus <- tm_map(corpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
mtx <- as.matrix(tdm)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAOyKb54nR25RK9jp9No1MaHgTzcsV0RiXaPW2Yg4YahZCXBqPtCqpLnSR1Q3xDa8gP6KUZAX0l3dpPlVhfQloK9TburKJJNJkt33aNUgCCIDsnQksokH6ZByUfaIVhg97ZCdMXTJp5FhKgheYO1aMvhsWfqm4jkRQhqTdatEzv81eKJZAom8ZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df), readerControl = list(language = "UTF-8"))
orpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
corpus <- tm_map(corpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
mtx <- as.matrix(tdm)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df), readerControl = list(language = "UTF-8"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
corpus <- tm_map(corpus, PlainTextDocument)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
mtx <- as.matrix(tdm)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAOyKb54nR25RK9jp9No1MaHgTzcsV0RiXaPW2Yg4YahZCXBqPtCqpLnSR1Q3xDa8gP6KUZAX0l3dpPlVhfQloK9TburKJJNJkt33aNUgCCIDsnQksokH6ZByUfaIVhg97ZCdMXTJp5FhKgheYO1aMvhsWfqm4jkRQhqTdatEzv81eKJZAom8ZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
colnames(msg_df) <- c(1:ncol(msg_df))
corpus <- Corpus(VectorSource(msg_df), readerControl = list(language = "UTF-8"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, 20)))
mtx <- as.matrix(tdm)
View(mtx)
View(msg_df)
corpus <- Corpus(DataFrame(msg_df), readerControl = list(language = "UTF-8"))
corpus <- Corpus(DataframeSource(msg_df), readerControl = list(language = "UTF-8"))
corpus <- Corpus(DataframeSource(data.frame(msg)), readerControl = list(language = "UTF-8"))
corpus <- Corpus(DataframeSource(data.frame(msg)))
msg_df <- data.frame(msg, stringsAsFactors=FALSE)
View(msg_df)
corpus <- Corpus(DataframeSource(data.frame(msg_df$msg)))
setwd("C:/Users/NDHUANG/workspace/dsprogramming/week_5")
source("tf-idf.R")
msg_df <- data.frame(msg, stringsAsFactors=FALSE)
corpus <- Corpus(DataframeSource(data.frame(msg_df$msg)))
docs = data.frame(
c("立找洗字人草鞋墩庄"),
c("北投保北投莊"),
c("邰北府淡水縣正堂"),
c("仝立找洗字人林"),
c("邰北縣奎府"),
c("立杜賣盡根絕田契字"),
c("仝立合約開鑿圳路字"),
c("立仝換斷田契字"),
c("立典大租契字竹塹社"),
c("邰灣布政使司"))
colnames(docs) <- c(1:ncol(docs))
corpus <- Corpus(VectorSource(docs))
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
View(corpus)
corpus <- Corpus(VectorSource(d.corpus))
corpus <- Corpus(VectorSource(corpus))
View(corpus)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
corpus <- Corpus(VectorSource(docs))
<
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
docs = data.frame(
c("立找洗字人草鞋墩庄"),
c("北投保北投莊"),
c("邰北府淡水縣正堂"),
c("仝立找洗字人林"),
c("邰北縣奎府"),
c("立杜賣盡根絕田契字"),
c("仝立合約開鑿圳路字"),
c("立仝換斷田契字"),
c("立典大租契字竹塹社"),
c("邰灣布政使司"))
colnames(docs) <- c(1:ncol(docs))
View(docs)
d.corpus <- Corpus(VectorSource(docs))
d.corpus <- tm_map(d.corpus, segmentCN, nature = TRUE)
d.corpus <- Corpus(VectorSource(d.corpus))
tdm <- TermDocumentMatrix(d.corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
d.corpus <- Corpus(VectorSource(docs))
d.corpus <- tm_map(d.corpus, segmentCN, nature = TRUE)
tdm <- TermDocumentMatrix(d.corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
View(d.corpus)
docs = data.frame(
c("立找洗字人草鞋墩庄"),
c("北投保北投莊"),
c("邰北府淡水縣正堂"),
c("仝立找洗字人林"),
c("邰北縣奎府"),
c("立杜賣盡根絕田契字"),
c("仝立合約開鑿圳路字"),
c("立仝換斷田契字"),
c("立典大租契字竹塹社"),
c("邰灣布政使司"))
d.corpus <- Corpus(VectorSource(docs))
d.corpus <- tm_map(d.corpus, segmentCN, nature = TRUE)
tdm <- TermDocumentMatrix(d.corpus, control = list(wordLengths = c(1,1)))
inspect(tdm)
library(tm)
library(tmcn)
library(jsonlite)
since <- "since=2018-06-01"
token <- "access_token=EAACEdEose0cBAOyKb54nR25RK9jp9No1MaHgTzcsV0RiXaPW2Yg4YahZCXBqPtCqpLnSR1Q3xDa8gP6KUZAX0l3dpPlVhfQloK9TburKJJNJkt33aNUgCCIDsnQksokH6ZByUfaIVhg97ZCdMXTJp5FhKgheYO1aMvhsWfqm4jkRQhqTdatEzv81eKJZAom8ZD"
params <- paste(since, token, sep="&")
url <- "https://graph.facebook.com/v3.0/136845026417486/feed?"
source <- paste(url, params, sep="")
raw <- fromJSON(source)
msg <- as.character(raw$data$message)
msg_df <- data.frame(as.list(msg), stringsAsFactors=FALSE)
View(msg_df)
colnames(msg_df) <- c(1:ncol(msg_df))
View(msg_df)
corpus <- Corpus(VectorSource(msg_df))
View(corpus)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, segmentCN, nature = TRUE)
corpus <- tm_map(corpus, function(word) {gsub("[A-Za-z0-9\r\n]", "", word)})
View(corpus)
tdm <- TermDocumentMatrix(corpus)
View(tdm)
